{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "py37  (Python3)",
      "language": "python",
      "name": "py37"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "LS_DS_413_Document_Classification_Lecture.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ryanleeallred/DS-Unit-4-Sprint-1-NLP/blob/main/module3-document-classification/LS_DS_413_Document_Classification_Lecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTBprEuZ9Dy0"
      },
      "source": [
        "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
        "\n",
        "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE,\" as well as your name and collaborators below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQah4gaB9Dy4"
      },
      "source": [
        "NAME = \"\"\n",
        "COLLABORATORS = \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMrXWykU9Dy5"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_M68YibX9Dy5"
      },
      "source": [
        "Lambda School Data Science\n",
        "\n",
        "*Unit 4, Sprint 1, Module 3*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHZrv4tW9Dy6"
      },
      "source": [
        "# Document Classification (Prepare)\n",
        "\n",
        "Today's guided module project will be different. You already know how to do classification. You already know how to extract features from documents. So? That means you're ready to combine and practice those skills in a Kaggle competition. We will open with a five-minute sprint explaining the competition and then give you twenty-five minutes to work. After those twenty-five minutes are up, I will give a five minute demo of an NLP technique to help you with document classification (*and **maybe** the competition*).\n",
        "\n",
        "Today's all about having fun and practicing your skills. The competition will begin.\n",
        "\n",
        "## Learning Objectives\n",
        "* <a href=\"#p1\">Part 1</a>: Text Feature Extraction & Classification Pipelines\n",
        "* <a href=\"#p2\">Part 2</a>: Latent Semantic Indexing\n",
        "* <a href=\"#p3\">Part 3</a>: Word Embeddings with Spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XScoEUv99Dy7"
      },
      "source": [
        "# Text Feature Extraction & Classification Pipelines (Learn)\n",
        "<a id=\"p1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkSI_Plc9Dy7"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Sklearn pipelines allow you to stitch together multiple components of a machine learning process. The idea is that you can pass your raw data and get predictions out of the pipeline. This ability to pass raw input and receive a prediction from a singular class makes pipelines well suited for production because you can pickle a pipeline without worrying about other data preprocessing steps. \n",
        "\n",
        "*Note:* Every time we call the pipeline during grid search, each component is fit again. The vectorizer (tf-idf) is transforming our entire vocabulary during each cross-validation fold. That transformation adds significant run time to our grid search. There *might* be interactions between the vectorizer and our classifier, so we estimate their performance together in the code below; however, your goal is to reduce run time. Train your vectorizer separately (i.e., out of the grid-searched pipeline). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHZMb2Y09Dy8"
      },
      "source": [
        "# Import Statements\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "75e17f01bde04e926a348a78398b3d55",
          "grade": false,
          "grade_id": "cell-2d860ec20fad5c0c",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "6Ua1RhmL9Dy9"
      },
      "source": [
        "# Dataset\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# 2 categories to class today\n",
        "categories = ['alt.atheism',\n",
        "              'talk.religion.misc']\n",
        "\n",
        "data = fetch_20newsgroups(subset='all', \n",
        "                          categories=categories)\n",
        "\n",
        "# prep data, instantiate a model, create pipeline object, and run a gridsearch \n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc-hr-collapsed": true,
        "id": "LBZRLavD9Dy-"
      },
      "source": [
        "## Follow Along \n",
        "\n",
        "What you should be doing now:\n",
        "1. Join the Kaggle Competition\n",
        "2. Download the data\n",
        "3. Train a model (try using the pipe method I just demoed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hTKWcNk9Dy-"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "You're trying to achieve 75% accuracy on your model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GSlcn6v9Dy_"
      },
      "source": [
        "## Latent Semantic Indexing (Learn)\n",
        "<a id=\"p2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fw_xdPoB9Dy_"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlBX3XsA9Dy_"
      },
      "source": [
        "![](https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1538411402/image3_maagmh.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJ-VenAG9DzA"
      },
      "source": [
        "from IPython.display import YouTubeVideo\n",
        "YouTubeVideo('OvzJiur55vo', width=1024, height=576)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iK8RiXoD9DzA"
      },
      "source": [
        "### First an example\n",
        "\n",
        "Before we apply Latent Semantic Analysis (LSA) in a pipeline, let's work through a simple example together to understand better how LSA works and develop an intuition along the way. \n",
        "\n",
        "First, if you haven't already, watch the short video provided above. Second, we will be implementing the example from the video in our notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnrHuGF99DzA"
      },
      "source": [
        "# Import\n",
        "\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "svd = TruncatedSVD(n_components=2, # number of topics to generate (also the size of the new feature space)\n",
        "                   algorithm='randomized',\n",
        "                   n_iter=10)\n",
        "\n",
        "# let's use the same data that was used in the video for consistency \n",
        "\n",
        "        # topic 1 data \n",
        "data = [\"pizza\", \n",
        "        \"hamburger\", \n",
        "        \"cookie\", \n",
        "        \"pizza hamburger cookie\",\n",
        "        # topic 2 data\n",
        "        \"ramen\", \n",
        "        \"sushi\", \n",
        "        \"ramen sushi\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e12d5a1e7f851ea065d8637f94beffea",
          "grade": false,
          "grade_id": "cell-74a4478e1d50513b",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "pvVIadlj9DzB"
      },
      "source": [
        "# CREATE Term-Frequency matrix \n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "9e77786ba4332af03128a0a4bd101d2b",
          "grade": false,
          "grade_id": "cell-cfc0e060c5491e20",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "4d1lyZi29DzB"
      },
      "source": [
        "# Use SVD to transform our Term-Frequency matrix into a Topic matrix with reduced dimensionality\n",
        "\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f4d45f8bf79393c9f929412dcd344bbb",
          "grade": false,
          "grade_id": "cell-f93d7b170dec7dfd",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "hErpTh0i9DzC"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ErmWH-k9DzC"
      },
      "source": [
        "Ok, now that we have gone through an example of applying LSA on a small dataset, let's apply it in a model-building pipeline. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "6e03b48a4a557a5d08513f0c7c3962e4",
          "grade": false,
          "grade_id": "cell-0ff7ed88cbc5eb32",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "KW2l08Gk9DzC"
      },
      "source": [
        "# build a pipeline, incorporate SVD, and run a gridsearch \n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc-hr-collapsed": true,
        "id": "tiu4rWhy9DzD"
      },
      "source": [
        "## Follow Along\n",
        "1. Join the Kaggle Competition\n",
        "2. Download the data\n",
        "3. Train a model and try: \n",
        "    - Creating a Text Extraction & Classification Pipeline\n",
        "    - Tune the pipeline with a `GridSearchCV` or `RandomizedSearchCV`\n",
        "    - Add some Latent Semantic Indexing (LSI) into your pipeline. *Note:* You can grid search a nested pipeline, but you have to use double underscores, i.e., `lsi__svd__n_components`\n",
        "4. Submit to Kaggle \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7E7ZImI29DzD"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "Continue to apply Latent Semantic Indexing (LSI) to various datasets. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtCTBBi09DzD"
      },
      "source": [
        "# Word Embeddings with Spacy (Learn)\n",
        "<a id=\"p3\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2VhqLyo9DzD"
      },
      "source": [
        "# Overview"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "996d16297087ad9c8acbfbeb726b15bb",
          "grade": false,
          "grade_id": "cell-30f6f3d27deb63a3",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "1ZRyYjK79DzD"
      },
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "\n",
        "# build a model that is trained on word vectors \n",
        "\n",
        "# load in pre-trained w2v model \n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpOL45NA9DzE"
      },
      "source": [
        "## Follow Along"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQdXnQFt9DzE"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "What you should be doing now:\n",
        "1. Join the Kaggle Competition\n",
        "2. Download the data\n",
        "3. Train a model and try: \n",
        "    - Creating a Text Extraction & Classification Pipeline\n",
        "    - Tune the pipeline with a `GridSearchCV` or `RandomizedSearchCV`\n",
        "    - Add some Latent Semantic Indexing (LSI) into your pipeline. *Note:* You can grid search a nested pipeline, but you have to use double underscores, i.e., `lsi__svd__n_components`\n",
        "    - Try to extract word embeddings with Spacy and use those embeddings as your features for a classification model\n",
        "4. Submit to Kaggle. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3rU8RZB9DzE"
      },
      "source": [
        "# Review\n",
        "\n",
        "To review this module: \n",
        "* Continue working on the Kaggle competition\n",
        "* Find another text classification task to work on"
      ]
    }
  ]
}