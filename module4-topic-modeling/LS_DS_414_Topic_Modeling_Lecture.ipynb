{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_414_Topic_Modeling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "py37  (Python3)",
      "language": "python",
      "name": "py37"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ryanleeallred/DS-Unit-4-Sprint-1-NLP/blob/main/module4-topic-modeling/LS_DS_414_Topic_Modeling_Lecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOMBkFKn6uEK"
      },
      "source": [
        "# Topic Modeling (Prepare)\n",
        "\n",
        "On Monday, we talked about summarizing your documents using just token counts. Today, we're going to learn about a much more sophisticated approach - learning 'topics' from documents. Topics are a latent structure. They are not directly observable in the data, but we know they're there by reading them.\n",
        "\n",
        "> **latent**: existing but not yet developed or manifest; hidden or concealed.\n",
        "\n",
        "## Use Cases\n",
        "**Primary use case**: What are your documents about? Who might want to know that in the industry - \n",
        "* Identifying common themes in customer reviews\n",
        "* Grouping job ads into categories \n",
        "* Monitoring communications (Email - State Department, Google) \n",
        "\n",
        "## Learning Objectives\n",
        "*At the end of the lesson you should be able to:*\n",
        "* Part 1: Describe how an LDA Model works\n",
        "* Part 2: Build an LDA Model with Gensim\n",
        "* Part 3: Interpret LDA results & Select the appropriate number of topics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnyNg8sQ6uEL"
      },
      "source": [
        "import re\n",
        "import NumPy as np\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from pandarallel import pandarallel\n",
        "\n",
        "\n",
        "import spacy\n",
        "spacy.util.fix_random_seed(0)\n",
        "\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim \n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbZjG6U86uEe"
      },
      "source": [
        "# Part 1: Describe How an LDA Model Works\n",
        "\n",
        "We will focus on a high level of understanding of how LDA works, meaning we will focus on \"what it does\" instead of \"how it does it.\" I realize that this may be unsatisfying, so I've included some resources that serve as a prerequisite for understanding how LDA works at a mathematical level. \n",
        "\n",
        "LDA is a [**Probabilistic Graphical Model (PGM)**](https://en.wikipedia.org/wiki/Graphical_model)\n",
        "\n",
        "PGM is represented by a graph that expresses the conditional dependence structure between random variables. Here's the LDA representation dependency graph: \n",
        "\n",
        "![](https://filebox.ece.vt.edu/~s14ece6504/projects/alfadda_topic/main_figure_3.png)\n",
        "\n",
        "These images are communicating the hierarchical dependency between probability distributions and their parameters. This is an application of Bayesian Probability - on steroids. \n",
        "\n",
        "\n",
        "To understand how LDA works, one must first understand how PGM works. If this is something that you're interested in learning more about, here are some resources: \n",
        "\n",
        "This Github repo has transformed a textbook into a collection of Jupyter Notebooks. This repo is called  [**\"Bayesian Methods for Hackers\"**](https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers) The cool thing about this repo is that each chapter has the same material covered in several notebooks. Still each notebook is written in a different python package: **PyMC2, PyMC3, Pyro, and Tensorflow Probability.** So you can even learn a new library if you want or stick to what you know. \n",
        "\n",
        "[**Pyro**](https://pyro.ai/) is considered a very powerful probabilistic programming library that combines probabilistic programming with deep learning. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfTjp3md4mlw"
      },
      "source": [
        "### Resources for LDA\n",
        "\n",
        "[**Your Guide to Latent Dirichlet Allocation**](https://medium.com/@lettier/how-does-lda-work-ill-explain-using-emoji-108abf40fa7d) Here's a medium article that works through an example of LDA. This article is useful if you'd like to get exposure to LDA outside of this notebook.\n",
        "\n",
        "[**LDA Topic Modeling**](https://lettier.com/projects/lda-topic-modeling/) This interactive data visualization tool allows us to explore a simple and visual example of LDA. We'll be using this to learn about LDA in class. \n",
        "\n",
        "[**Topic Modeling with Gensim**](https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/) This is an example of implementing LDA using the same dataset we are using in the guided project.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eMW_d144mlw"
      },
      "source": [
        "# Problem Statement\n",
        "\n",
        "We are going to load some emails. Those emails belong to topics; however, those topics are hierarchical.\n",
        "\n",
        "    sci\n",
        "        \\_ electronics, space\n",
        "\n",
        "\n",
        "    talk\n",
        "        \\_politics \n",
        "                  \\_ guns, middle east\n",
        "              \n",
        "So what's the best way to categorize these emails - is it between science and talk? \n",
        "\n",
        "Is it between electronics, space, guns, and the Middle East? \n",
        "\n",
        "The Middle East is a pretty broad topic in and of itself; should that topic be broken down into further sub-topics?\n",
        "\n",
        "\n",
        "Let's learn about Topic Modeling and how it can help us answer these questions!\n",
        "\n",
        "### Load Email Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sVMV1764mlx"
      },
      "source": [
        "# notice that the categories are hierarchical\n",
        "# so there is a sense in which we 2 topics but also as many as 4 topics  \n",
        "categories = ['sci.electronics', 'sci.space', \n",
        "              'talk.politics.guns', 'talk.politics.mideast']\n",
        "data = fetch_20newsgroups(categories=categories)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "53a96e4bd0cab21c17f1baa88b5699d8",
          "grade": false,
          "grade_id": "cell-3240b1c818e9dcc2",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "_72aSzM54mly"
      },
      "source": [
        "# create X and Y from data\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaMsy1XAGLxc"
      },
      "source": [
        "# loda data into a dataframe \n",
        "\n",
        "\n",
        "data = {\n",
        "    'content': X,\n",
        "    'target': Y,\n",
        "    'target_names': [target_names[target_id] for target_id in Y]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data=data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "id": "na2bkOcFGter",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "839eeeb300650deafafc2dcd19a9e597",
          "grade": false,
          "grade_id": "cell-6ce62b4e40acee5f",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ajw_ZCmS4ml0"
      },
      "source": [
        "# clean our text data and save it to a new column\n",
        "df[\"clean_data\"] = df[\"content\"].apply(clean_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qkez8N9n4ml1"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwKMSm8e4ml1"
      },
      "source": [
        "### Create Tokens \n",
        "\n",
        "We must first create tokens before using the Gemsim library to create bag-of-words vectors in precisely the right way that the LDA model wants. \n",
        "\n",
        "Let’s use spaCy to create some lemmas. But first, let’s initialize our multi-processing library pandarallel, which will empower us to use the same dataframe that our data is stored in but create tokens in parallel to save time.\n",
        "\n",
        "Here's the documentation for [**pandarallel**](https://github.com/nalepae/pandarallel)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1UD0Zw24ml1"
      },
      "source": [
        "# we mush initalize pandarallel before we can use it\n",
        "pandarallel.initialize(progress_bar=True, nb_workers=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFrp8Eu-4ml2"
      },
      "source": [
        "# load in our spaCy language model\n",
        "nlp = spacy.load(\"en_core_web_lg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EBPQXqEKE9P"
      },
      "source": [
        "%%time\n",
        "# create our tokens in the form of lemmas \n",
        "df['lemmas'] = df['clean_data'].parallel_apply(lambda x: [token.lemma_ for token in nlp(x) if (token.is_stop != True) and (token.is_punct != True)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gg1ir0IS4ml3"
      },
      "source": [
        "### Take a Look at Our Lemmas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pffBFauE4ml3"
      },
      "source": [
        "# print out the lemmas from the first article"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vuc3svQ4ml3"
      },
      "source": [
        "### Filter Out Low-Quality Lemmas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEbvTkZd4ml3"
      },
      "source": [
        "def filter_lemmas(lemmas):\n",
        "    \"\"\"\n",
        "    Filter out any lemmas that are 2 characters or smaller\n",
        "    \"\"\"\n",
        "    return [lemmas for lemmas in lemmas if len(lemmas) > 2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cz2ixyfe4ml3"
      },
      "source": [
        "# apply filter_lemmas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEGi2Kf74ml4"
      },
      "source": [
        "### The Two Main Inputs to the LDA Topic Model are the Dictionary (id2word) and the Corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "id": "1klqRpqtJxWc",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "dd66ab4eff9f69a3e9be0aa57fdc2598",
          "grade": false,
          "grade_id": "cell-79d38b90e5c6e38b",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "# Create Dictionary\n",
        "\n",
        "# Term Document Frequency\n",
        "\n",
        "# stores (token id, token count) for each doc in the corpus\n",
        "\n",
        "# Human readable format of the corpus (term-frequency)\n",
        "\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZQInqL-4ml4"
      },
      "source": [
        "# Part 2: Estimate an LDA Model with Gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIRjvQkR4ml4"
      },
      "source": [
        " ### Train an LDA model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fasvjf0VLQ2a"
      },
      "source": [
        "### This cell runs the single-processor version of the model (slower)\n",
        "# %%time\n",
        "# lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "#                                            id2word=id2word,\n",
        "#                                            num_topics=20, \n",
        "#                                            chunksize=100,\n",
        "#                                            passes=10,\n",
        "#                                            per_word_topics=True)\n",
        "# lda_model.save('lda_model.model')\n",
        "# # https://radimrehurek.com/gensim/models/ldamodel.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aAcjLEy4ml4"
      },
      "source": [
        "%%time\n",
        "\n",
        "num_topics = 2\n",
        "### This cell runs the multi-processor version of the model (faster)\n",
        "lda_multicore_2_topics = gensim.models.ldamulticore.LdaMulticore(corpus=corpus,\n",
        "                                                        id2word=id2word,\n",
        "                                                        num_topics=num_topics, \n",
        "                                                        chunksize=100,\n",
        "                                                        passes=10,# runtime related parameter\n",
        "                                                        per_word_topics=True,\n",
        "                                                        workers=10, # runtime related parameter\n",
        "                                                        random_state=1234, \n",
        "                                                        iterations=20) # runtime related parameter\n",
        "\n",
        "num_topics = 6\n",
        "### This cell runs the multi-processor version of the model (faster)\n",
        "lda_multicore_6_topics = gensim.models.ldamulticore.LdaMulticore(corpus=corpus,\n",
        "                                                        id2word=id2word,\n",
        "                                                        num_topics=num_topics, \n",
        "                                                        chunksize=100,\n",
        "                                                        passes=10,# runtime related parameter\n",
        "                                                        per_word_topics=True,\n",
        "                                                        workers=10, # runtime related parameter\n",
        "                                                        random_state=1234, \n",
        "                                                        iterations=20) # runtime related parameter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vHFfLL24ml5"
      },
      "source": [
        "from gensim import models\n",
        "#lda_multicore =  models.LdaModel.load('lda_multicore.model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2m9L5xnT4ml5"
      },
      "source": [
        "# Part 3: Interpret LDA Results & Select the Appropriate Number of Topics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYXi480VLaHK"
      },
      "source": [
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim.prepare(lda_multicore_2_topics, corpus, id2word)\n",
        "vis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPmLG2df4ml5"
      },
      "source": [
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim.prepare(lda_multicore_6_topics, corpus, id2word)\n",
        "vis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZtxovPz4ml5"
      },
      "source": [
        "## What is Topic Coherence?\n",
        "Topic Coherence measures the score of a single topic by **measuring the degree of semantic similarity between high-scoring words in the topic**. These measurements help distinguish between topics that are semantically interpretable topics and topics that are artifacts of statistical inference.\n",
        "\n",
        "\n",
        "A set of statements or facts is said to be coherent if they support each other. Thus, a coherent fact set can be interpreted in a context that covers all or most of the facts. An example of a coherent fact set is **“the game is a team sport,”** **“the game is played with a ball,”** **“the game demands great physical efforts.”**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjtXk8J3LaXC"
      },
      "source": [
        "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
        "    \"\"\"\n",
        "    Compute c_v coherence for various number of topics\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    dictionary: Gensim dictionary\n",
        "    corpus: Gensim corpus\n",
        "    texts: List of input texts\n",
        "    limit: Max num of topics\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    model_list: List of LDA topic models\n",
        "    coherence_values: Coherence values corresponding to the LDA model with a respective number of topics\n",
        "    \"\"\"\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for num_topics in range(start, limit, step):\n",
        "        model = gensim.models.ldamulticore.LdaMulticore(corpus=corpus,\n",
        "                                                        id2word=id2word,\n",
        "                                                        num_topics=num_topics, \n",
        "                                                        chunksize=100,\n",
        "                                                        passes=10,\n",
        "                                                        random_state=1234,\n",
        "                                                        per_word_topics=True,\n",
        "                                                        workers=10)\n",
        "        model_list.append(model)\n",
        "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "\n",
        "    return model_list, coherence_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dUQ9mdx4ml6"
      },
      "source": [
        "%%time\n",
        "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=df['lemmas'], start=2, limit=10, step=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vp1sohpW4ml6"
      },
      "source": [
        "start=2; limit=10;  step=1;\n",
        "x = range(start, limit, step)\n",
        "\n",
        "plt.figure(figsize=(20,5))\n",
        "plt.grid()\n",
        "plt.title(\"Coherence Score vs. Number of Topics\")\n",
        "plt.xticks(x)\n",
        "plt.plot(x, coherence_values, \"-o\")\n",
        "\n",
        "plt.xlabel(\"Num Topics\")\n",
        "plt.ylabel(\"Coherence score\")\n",
        "\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71pQkahc4ml6"
      },
      "source": [
        "### Index for Model \n",
        "\n",
        "Due to the probabilistic nature of this model, the modeling results can and usually do vary. Despite this, we will select 8 as the number of topics even if this model doesn't show 8 as having the highest coherence score. Also, we need to ask ourselves how many topics we want for our corpus, even if it doesn’t."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBZ1pUzy4ml6"
      },
      "source": [
        "lda_trained_model = model_list[-2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGraSY454ml6"
      },
      "source": [
        "lda_trained_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8jfaXQx4ml6"
      },
      "source": [
        "# visualize the 3 topics \n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim.prepare(lda_trained_model, corpus, id2word)\n",
        "vis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kB4n92F64ml7"
      },
      "source": [
        "## Create a Topic ID/Name Dictionary \n",
        "\n",
        "When populating your topic id/name dictionary, use the index ordering as shown in the viz tool. \n",
        "\n",
        "We'll use a function to map the the viz tool index ordering with the train LDA model ordering. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f578664ea9e81e93977a50dee417f2df",
          "grade": false,
          "grade_id": "cell-777be73d0d1455f0",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "4SQ7uFBN4ml7"
      },
      "source": [
        "# keys - use topic ids from pyLDAvis visualization \n",
        "# values - topic names that you create \n",
        "# save dictionary to `vis_topic_name_dict`\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPKZ_3Mx4ml7"
      },
      "source": [
        "def get_topic_id_lookup_dict(vis, vis_topic_name_dict):\n",
        "    \"\"\"\n",
        "    The starting index and the ordering of topic ids between the trained LDA model and the viz tool are different. So we need to create a dictionary that maps the correct association between topic ids from both sources. \n",
        "    \"\"\"\n",
        "    # value is order of topic ids according to pyLDAvis tool \n",
        "    # key is order of topic ids according to lda model\n",
        "    model_vis_tool_topic_id_lookup = vis.topic_coordinates.topics.to_dict()\n",
        "\n",
        "    # invert dictionary so that \n",
        "    # key is order of topic ids according to pyLDAvis tool \n",
        "    # value is order of topic ids according to lda model\n",
        "    topic_id_lookup =  {v:k for k, v in model_vis_tool_topic_id_lookup.items()}\n",
        "    \n",
        "    # iterate through topic_id_lookup and index vis_topic_name_dict using the keys \n",
        "    # in order to swap the viz topic ids in vis_topic_name_dict for the lda model topic ids \n",
        "    return {v:vis_topic_name_dict[k]  for k, v in topic_id_lookup.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "92d9c30261d1cdb278e73f457c166609",
          "grade": false,
          "grade_id": "cell-0718245fd125b36e",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "p8TQWndq4ml7"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AHcrHQC4ml7"
      },
      "source": [
        "topic_name_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyVT9MSw4ml7"
      },
      "source": [
        "## Assign Each Document a Topic Name\n",
        "\n",
        "Now that we have a topic id/name look-up dict aligned with the index ordering of the trained LDA model, we can give each topic a topic name. \n",
        "\n",
        "The function below has been given to you. However, you are highly encouraged to read through it and make sure that you understand what it is doing each step of the way. An excellent way to do this is to copy and paste the code inside the function into a new cell, comment out all the lines of code and line by line, uncomment the code and see the output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJV8ncbK4ml7"
      },
      "source": [
        "def get_topic_ids_for_docs(lda_model, corpus):\n",
        "    \n",
        "    \"\"\"\n",
        "    Passes a Bag-of-Words vector into a trained LDA model to get the topic id of that document. \n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    lda_model: Gensim object\n",
        "        Must be a trained model \n",
        "        \n",
        "    corpus: nested lists of tuples, \n",
        "        i.e. [[(),(), ..., ()], [(),(), ..., ()], ..., [(),(), ..., ()]]\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    topic_id_list: list\n",
        "        Contains topic ids for all document vectors in corpus \n",
        "    \"\"\"\n",
        "    \n",
        "    # store topic ids for each document\n",
        "    doc_topic_ids = []\n",
        "\n",
        "    # iterature through the bow vectors for each doc\n",
        "    for doc_bow in corpus:\n",
        "        \n",
        "        # store the topic ids for the doc\n",
        "        topic_ids = []\n",
        "        # store the topic probabilities for the doc\n",
        "        topic_probs = []\n",
        "\n",
        "        # list of tuples\n",
        "        # each tuple has a topic id and the prob that the doc belongs to that topic \n",
        "        topic_id_prob_tuples = lda_trained_model.get_document_topics(doc_bow)\n",
        "        \n",
        "        # iterate through the topic id/prob pairs \n",
        "        for topic_id_prob in topic_id_prob_tuples:\n",
        "            \n",
        "            # index for topic id\n",
        "            topic_id = topic_id_prob[0]\n",
        "            # index for prob that doc belongs that the corresponding topic\n",
        "            topic_prob = topic_id_prob[1]\n",
        "\n",
        "            # store all topic ids for doc\n",
        "            topic_ids.append(topic_id)\n",
        "            # store all topic probs for doc\n",
        "            topic_probs.append(topic_prob)\n",
        "\n",
        "        # get index for largest prob score\n",
        "        max_topic_prob_ind = np.argmax(topic_probs)\n",
        "        # get corresponding topic id\n",
        "        max_prob_topic_id = topic_ids[max_topic_prob_ind]\n",
        "        # store topic id that had the highest prob for doc being a memebr of that topic\n",
        "        doc_topic_ids.append(max_prob_topic_id)\n",
        "        \n",
        "    return doc_topic_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4Dh3wvy4ml8"
      },
      "source": [
        "# get the topic id for each doc in the corpus \n",
        "topic_id_list = get_topic_ids_for_docs(lda_trained_model, corpus)\n",
        "\n",
        "# creat a feature for document's topic id\n",
        "df[\"topic_id\"] = topic_id_list\n",
        "\n",
        "# iterate through the topic id and use the lookup table to assign each document with a topic name\n",
        "df[\"new_topic_name\"] = df[\"topic_id\"].apply(lambda topic_id: topic_name_dict[topic_id])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQtte8h74ml8"
      },
      "source": [
        "# cool! so now all of our documents have topic ids and names \n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4bsiK-a4ml8"
      },
      "source": [
        "# you can mask for all Space articles \n",
        "science_mask = df.topic_id == 3\n",
        "df[science_mask]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gqx_3vyq4ml8"
      },
      "source": [
        "-----\n",
        "\n",
        "## Where Do We Go From Here?\n",
        "\n",
        "What exactly did we accomplish?\n",
        "\n",
        "Outside of this guided project (i.e., in your job), you may or may not have access to existing article topic names like we did with this data set, which means that we won't always have a point of reference to \"check our answers.\" So let's explore two possible situations in which you might find yourself using this Unsupervised Learning Topic Model. \n",
        "\n",
        "### 1. You have access to existing document topic labels\n",
        "\n",
        "In this case, why would we bother with Topic Modeling? It could be the case that the current topic labels are not helpful for whatever task you're working on. For instance, our email dataset here has topic names; however, those topic labels are hierarchical, which doesn't suit your needs for some reason. So one option is to generate new labels that suit your needs (like we did here). \n",
        "\n",
        "### 2. Your corpus doesn't have any document topic labels\n",
        "\n",
        "In this case, you don't have any pre-existing topic labels. Maybe you work at Indeed or LinkedIn or Google, and your job is to bring some structure to a huge collection of emails and messages that aren't labeled in any meaningful way, so it isn't easy to sort through these documents. This is a perfect use case of Topic Modeling. After you apply topic modeling, you’ll have organized your emails into broad categories. You can start structuring and then analyze your corpus and maybe even build a supervised learning model to predict the document's topic!"
      ]
    }
  ]
}